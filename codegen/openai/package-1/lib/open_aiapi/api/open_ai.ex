# NOTE: This file is auto generated by OpenAPI Generator 6.6.0 (https://openapi-generator.tech).
# Do not edit this file manually.

defmodule OpenAIAPI.Api.OpenAI do
  @moduledoc """
  API calls for all endpoints tagged `OpenAI`.
  """

  alias OpenAIAPI.Connection
  import OpenAIAPI.RequestBuilder

  @doc """
  Immediately cancel a fine-tune job. 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `fine_tune_id` (String.t): The ID of the fine-tune job to cancel 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.FineTune.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec cancel_fine_tune(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.FineTune.t} | {:error, Tesla.Env.t}
  def cancel_fine_tune(connection, fine_tune_id, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/fine-tunes/#{fine_tune_id}/cancel")
      |> ensure_body()
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.FineTune{}}
    ])
  end

  @doc """
  Answers the specified question using the provided documents and examples.  The endpoint first [searches](/docs/api-reference/searches) over provided documents or files to find relevant context. The relevant context is combined with the provided examples and question to create the prompt for [completion](/docs/api-reference/completions). 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_answer_request` (CreateAnswerRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateAnswerResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_answer(Tesla.Env.client, OpenAIAPI.Model.CreateAnswerRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateAnswerResponse.t} | {:error, Tesla.Env.t}
  def create_answer(connection, create_answer_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/answers")
      |> add_param(:body, :body, create_answer_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateAnswerResponse{}}
    ])
  end

  @doc """
  Creates a completion for the chat message

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_chat_completion_request` (CreateChatCompletionRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateChatCompletionResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_chat_completion(Tesla.Env.client, OpenAIAPI.Model.CreateChatCompletionRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateChatCompletionResponse.t} | {:error, Tesla.Env.t}
  def create_chat_completion(connection, create_chat_completion_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/chat/completions")
      |> add_param(:body, :body, create_chat_completion_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateChatCompletionResponse{}}
    ])
  end

  @doc """
  Classifies the specified `query` using provided examples.  The endpoint first [searches](/docs/api-reference/searches) over the labeled examples to select the ones most relevant for the particular query. Then, the relevant examples are combined with the query to construct a prompt to produce the final label via the [completions](/docs/api-reference/completions) endpoint.  Labeled examples can be provided via an uploaded `file`, or explicitly listed in the request using the `examples` parameter for quick tests and small scale use cases. 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_classification_request` (CreateClassificationRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateClassificationResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_classification(Tesla.Env.client, OpenAIAPI.Model.CreateClassificationRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateClassificationResponse.t} | {:error, Tesla.Env.t}
  def create_classification(connection, create_classification_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/classifications")
      |> add_param(:body, :body, create_classification_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateClassificationResponse{}}
    ])
  end

  @doc """
  Creates a completion for the provided prompt and parameters

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_completion_request` (CreateCompletionRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateCompletionResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_completion(Tesla.Env.client, OpenAIAPI.Model.CreateCompletionRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateCompletionResponse.t} | {:error, Tesla.Env.t}
  def create_completion(connection, create_completion_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/completions")
      |> add_param(:body, :body, create_completion_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateCompletionResponse{}}
    ])
  end

  @doc """
  Creates a new edit for the provided input, instruction, and parameters.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_edit_request` (CreateEditRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateEditResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_edit(Tesla.Env.client, OpenAIAPI.Model.CreateEditRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateEditResponse.t} | {:error, Tesla.Env.t}
  def create_edit(connection, create_edit_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/edits")
      |> add_param(:body, :body, create_edit_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateEditResponse{}}
    ])
  end

  @doc """
  Creates an embedding vector representing the input text.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_embedding_request` (CreateEmbeddingRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateEmbeddingResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_embedding(Tesla.Env.client, OpenAIAPI.Model.CreateEmbeddingRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateEmbeddingResponse.t} | {:error, Tesla.Env.t}
  def create_embedding(connection, create_embedding_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/embeddings")
      |> add_param(:body, :body, create_embedding_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateEmbeddingResponse{}}
    ])
  end

  @doc """
  Upload a file that contains document(s) to be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit. 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `file` (String.t): Name of the [JSON Lines](https://jsonlines.readthedocs.io/en/latest/) file to be uploaded.  If the `purpose` is set to \\\"fine-tune\\\", each line is a JSON record with \\\"prompt\\\" and \\\"completion\\\" fields representing your [training examples](/docs/guides/fine-tuning/prepare-training-data). 
  - `purpose` (String.t): The intended purpose of the uploaded documents.  Use \\\"fine-tune\\\" for [Fine-tuning](/docs/api-reference/fine-tunes). This allows us to validate the format of the uploaded file. 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.OpenAiFile.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_file(Tesla.Env.client, String.t, String.t, keyword()) :: {:ok, OpenAIAPI.Model.OpenAiFile.t} | {:error, Tesla.Env.t}
  def create_file(connection, file, purpose, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/files")
      |> add_param(:file, :file, file)
      |> add_param(:form, :purpose, purpose)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.OpenAiFile{}}
    ])
  end

  @doc """
  Creates a job that fine-tunes a specified model from a given dataset.  Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.  [Learn more about Fine-tuning](/docs/guides/fine-tuning) 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_fine_tune_request` (CreateFineTuneRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.FineTune.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_fine_tune(Tesla.Env.client, OpenAIAPI.Model.CreateFineTuneRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.FineTune.t} | {:error, Tesla.Env.t}
  def create_fine_tune(connection, create_fine_tune_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/fine-tunes")
      |> add_param(:body, :body, create_fine_tune_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.FineTune{}}
    ])
  end

  @doc """
  Creates an image given a prompt.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_image_request` (CreateImageRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.ImagesResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_image(Tesla.Env.client, OpenAIAPI.Model.CreateImageRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.ImagesResponse.t} | {:error, Tesla.Env.t}
  def create_image(connection, create_image_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/images/generations")
      |> add_param(:body, :body, create_image_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ImagesResponse{}}
    ])
  end

  @doc """
  Creates an edited or extended image given an original image and a prompt.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `image` (String.t): The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.
  - `prompt` (String.t): A text description of the desired image(s). The maximum length is 1000 characters.
  - `opts` (keyword): Optional parameters
    - `:mask` (String.t): An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as `image`.
    - `:n` (integer()): The number of images to generate. Must be between 1 and 10.
    - `:size` (String.t): The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
    - `:response_format` (String.t): The format in which the generated images are returned. Must be one of `url` or `b64_json`.
    - `:user` (String.t): A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 

  ### Returns

  - `{:ok, OpenAIAPI.Model.ImagesResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_image_edit(Tesla.Env.client, String.t, String.t, keyword()) :: {:ok, OpenAIAPI.Model.ImagesResponse.t} | {:error, Tesla.Env.t}
  def create_image_edit(connection, image, prompt, opts \\ []) do
    optional_params = %{
      :mask => :form,
      :n => :form,
      :size => :form,
      :response_format => :form,
      :user => :form
    }

    request =
      %{}
      |> method(:post)
      |> url("/images/edits")
      |> add_param(:file, :image, image)
      |> add_param(:form, :prompt, prompt)
      |> add_optional_params(optional_params, opts)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ImagesResponse{}}
    ])
  end

  @doc """
  Creates a variation of a given image.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `image` (String.t): The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.
  - `opts` (keyword): Optional parameters
    - `:n` (integer()): The number of images to generate. Must be between 1 and 10.
    - `:size` (String.t): The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
    - `:response_format` (String.t): The format in which the generated images are returned. Must be one of `url` or `b64_json`.
    - `:user` (String.t): A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 

  ### Returns

  - `{:ok, OpenAIAPI.Model.ImagesResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_image_variation(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.ImagesResponse.t} | {:error, Tesla.Env.t}
  def create_image_variation(connection, image, opts \\ []) do
    optional_params = %{
      :n => :form,
      :size => :form,
      :response_format => :form,
      :user => :form
    }

    request =
      %{}
      |> method(:post)
      |> url("/images/variations")
      |> add_param(:file, :image, image)
      |> add_optional_params(optional_params, opts)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ImagesResponse{}}
    ])
  end

  @doc """
  Classifies if text violates OpenAI's Content Policy

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `create_moderation_request` (CreateModerationRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateModerationResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_moderation(Tesla.Env.client, OpenAIAPI.Model.CreateModerationRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateModerationResponse.t} | {:error, Tesla.Env.t}
  def create_moderation(connection, create_moderation_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/moderations")
      |> add_param(:body, :body, create_moderation_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateModerationResponse{}}
    ])
  end

  @doc """
  The search endpoint computes similarity scores between provided query and documents. Documents can be passed directly to the API if there are no more than 200 of them.  To go beyond the 200 document limit, documents can be processed offline and then used for efficient retrieval at query time. When `file` is set, the search endpoint searches over all the documents in the given file and returns up to the `max_rerank` number of documents. These documents will be returned along with their search scores.  The similarity score is a positive score that usually ranges from 0 to 300 (but can sometimes go higher), where a score above 200 usually means the document is semantically similar to the query. 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `engine_id` (String.t): The ID of the engine to use for this request.  You can select one of `ada`, `babbage`, `curie`, or `davinci`.
  - `create_search_request` (CreateSearchRequest): 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateSearchResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_search(Tesla.Env.client, String.t, OpenAIAPI.Model.CreateSearchRequest.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateSearchResponse.t} | {:error, Tesla.Env.t}
  def create_search(connection, engine_id, create_search_request, _opts \\ []) do
    request =
      %{}
      |> method(:post)
      |> url("/engines/#{engine_id}/search")
      |> add_param(:body, :body, create_search_request)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateSearchResponse{}}
    ])
  end

  @doc """
  Transcribes audio into the input language.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `file` (String.t): The audio file to transcribe, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. 
  - `model` (String.t): ID of the model to use. Only `whisper-1` is currently available. 
  - `opts` (keyword): Optional parameters
    - `:prompt` (String.t): An optional text to guide the model's style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language. 
    - `:response_format` (String.t): The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. 
    - `:temperature` (float()): The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
    - `:language` (String.t): The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. 

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateTranscriptionResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_transcription(Tesla.Env.client, String.t, String.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateTranscriptionResponse.t} | {:error, Tesla.Env.t}
  def create_transcription(connection, file, model, opts \\ []) do
    optional_params = %{
      :prompt => :form,
      :response_format => :form,
      :temperature => :form,
      :language => :form
    }

    request =
      %{}
      |> method(:post)
      |> url("/audio/transcriptions")
      |> add_param(:file, :file, file)
      |> add_param(:form, :model, model)
      |> add_optional_params(optional_params, opts)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateTranscriptionResponse{}}
    ])
  end

  @doc """
  Translates audio into into English.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `file` (String.t): The audio file to translate, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. 
  - `model` (String.t): ID of the model to use. Only `whisper-1` is currently available. 
  - `opts` (keyword): Optional parameters
    - `:prompt` (String.t): An optional text to guide the model's style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English. 
    - `:response_format` (String.t): The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. 
    - `:temperature` (float()): The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 

  ### Returns

  - `{:ok, OpenAIAPI.Model.CreateTranslationResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec create_translation(Tesla.Env.client, String.t, String.t, keyword()) :: {:ok, OpenAIAPI.Model.CreateTranslationResponse.t} | {:error, Tesla.Env.t}
  def create_translation(connection, file, model, opts \\ []) do
    optional_params = %{
      :prompt => :form,
      :response_format => :form,
      :temperature => :form
    }

    request =
      %{}
      |> method(:post)
      |> url("/audio/translations")
      |> add_param(:file, :file, file)
      |> add_param(:form, :model, model)
      |> add_optional_params(optional_params, opts)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.CreateTranslationResponse{}}
    ])
  end

  @doc """
  Delete a file.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `file_id` (String.t): The ID of the file to use for this request
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.DeleteFileResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec delete_file(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.DeleteFileResponse.t} | {:error, Tesla.Env.t}
  def delete_file(connection, file_id, _opts \\ []) do
    request =
      %{}
      |> method(:delete)
      |> url("/files/#{file_id}")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.DeleteFileResponse{}}
    ])
  end

  @doc """
  Delete a fine-tuned model. You must have the Owner role in your organization.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `model` (String.t): The model to delete
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.DeleteModelResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec delete_model(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.DeleteModelResponse.t} | {:error, Tesla.Env.t}
  def delete_model(connection, model, _opts \\ []) do
    request =
      %{}
      |> method(:delete)
      |> url("/models/#{model}")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.DeleteModelResponse{}}
    ])
  end

  @doc """
  Returns the contents of the specified file

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `file_id` (String.t): The ID of the file to use for this request
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, String.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec download_file(Tesla.Env.client, String.t, keyword()) :: {:ok, String.t} | {:error, Tesla.Env.t}
  def download_file(connection, file_id, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/files/#{file_id}/content")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, false}
    ])
  end

  @doc """
  Lists the currently available (non-finetuned) models, and provides basic information about each one such as the owner and availability.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.ListEnginesResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec list_engines(Tesla.Env.client, keyword()) :: {:ok, OpenAIAPI.Model.ListEnginesResponse.t} | {:error, Tesla.Env.t}
  def list_engines(connection, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/engines")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ListEnginesResponse{}}
    ])
  end

  @doc """
  Returns a list of files that belong to the user's organization.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.ListFilesResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec list_files(Tesla.Env.client, keyword()) :: {:ok, OpenAIAPI.Model.ListFilesResponse.t} | {:error, Tesla.Env.t}
  def list_files(connection, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/files")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ListFilesResponse{}}
    ])
  end

  @doc """
  Get fine-grained status updates for a fine-tune job. 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `fine_tune_id` (String.t): The ID of the fine-tune job to get events for. 
  - `opts` (keyword): Optional parameters
    - `:stream` (boolean()): Whether to stream events for the fine-tune job. If set to true, events will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available. The stream will terminate with a `data: [DONE]` message when the job is finished (succeeded, cancelled, or failed).  If set to false, only events generated so far will be returned. 

  ### Returns

  - `{:ok, OpenAIAPI.Model.ListFineTuneEventsResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec list_fine_tune_events(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.ListFineTuneEventsResponse.t} | {:error, Tesla.Env.t}
  def list_fine_tune_events(connection, fine_tune_id, opts \\ []) do
    optional_params = %{
      :stream => :query
    }

    request =
      %{}
      |> method(:get)
      |> url("/fine-tunes/#{fine_tune_id}/events")
      |> add_optional_params(optional_params, opts)
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ListFineTuneEventsResponse{}}
    ])
  end

  @doc """
  List your organization's fine-tuning jobs 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.ListFineTunesResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec list_fine_tunes(Tesla.Env.client, keyword()) :: {:ok, OpenAIAPI.Model.ListFineTunesResponse.t} | {:error, Tesla.Env.t}
  def list_fine_tunes(connection, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/fine-tunes")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ListFineTunesResponse{}}
    ])
  end

  @doc """
  Lists the currently available models, and provides basic information about each one such as the owner and availability.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.ListModelsResponse.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec list_models(Tesla.Env.client, keyword()) :: {:ok, OpenAIAPI.Model.ListModelsResponse.t} | {:error, Tesla.Env.t}
  def list_models(connection, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/models")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.ListModelsResponse{}}
    ])
  end

  @doc """
  Retrieves a model instance, providing basic information about it such as the owner and availability.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `engine_id` (String.t): The ID of the engine to use for this request 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.Engine.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec retrieve_engine(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.Engine.t} | {:error, Tesla.Env.t}
  def retrieve_engine(connection, engine_id, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/engines/#{engine_id}")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.Engine{}}
    ])
  end

  @doc """
  Returns information about a specific file.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `file_id` (String.t): The ID of the file to use for this request
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.OpenAiFile.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec retrieve_file(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.OpenAiFile.t} | {:error, Tesla.Env.t}
  def retrieve_file(connection, file_id, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/files/#{file_id}")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.OpenAiFile{}}
    ])
  end

  @doc """
  Gets info about the fine-tune job.  [Learn more about Fine-tuning](/docs/guides/fine-tuning) 

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `fine_tune_id` (String.t): The ID of the fine-tune job 
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.FineTune.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec retrieve_fine_tune(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.FineTune.t} | {:error, Tesla.Env.t}
  def retrieve_fine_tune(connection, fine_tune_id, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/fine-tunes/#{fine_tune_id}")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.FineTune{}}
    ])
  end

  @doc """
  Retrieves a model instance, providing basic information about the model such as the owner and permissioning.

  ### Parameters

  - `connection` (OpenAIAPI.Connection): Connection to server
  - `model` (String.t): The ID of the model to use for this request
  - `opts` (keyword): Optional parameters

  ### Returns

  - `{:ok, OpenAIAPI.Model.Model.t}` on success
  - `{:error, Tesla.Env.t}` on failure
  """
  @spec retrieve_model(Tesla.Env.client, String.t, keyword()) :: {:ok, OpenAIAPI.Model.Model.t} | {:error, Tesla.Env.t}
  def retrieve_model(connection, model, _opts \\ []) do
    request =
      %{}
      |> method(:get)
      |> url("/models/#{model}")
      |> Enum.into([])

    connection
    |> Connection.request(request)
    |> evaluate_response([
      {200, %OpenAIAPI.Model.Model{}}
    ])
  end
end
